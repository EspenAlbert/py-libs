{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Get Started","text":""},{"location":"#py-libs-drastic-changes-coming-for-v100","title":"py-libs (drastic changes coming for v1.0.0)","text":"<ul> <li>An experiment for sharing python packages</li> <li>~~compose_chart_export~~<ul> <li><code>pip install compose-chart-export</code></li> </ul> </li> <li>~~docker_compose_parser~~<ul> <li><code>pip install docker-compose-parser</code></li> </ul> </li> <li>model_lib-pydantic base models with convenient dump methods<ul> <li><code>pip install model-lib</code></li> </ul> </li> <li>zero_lib-handy standalone scripts without 3rdparty dependencies<ul> <li><code>pip install zero-3rdparty</code></li> </ul> </li> </ul>"},{"location":"#hierarchy","title":"Hierarchy","text":"<pre><code>flowchart TD\n    model_lib --&gt; zero_3rdparty\n    compose_chart_export --&gt; model_lib\n    docker_compose_parser --&gt; model_lib\n    compose_chart_export --&gt; docker_compose_parser\n    pants_py_deploy --&gt; compose_chart_export\n    pants_py_deploy --&gt; docker_compose_parser\n\n    click zero_3rdparty href \"/py-libs/zero-3rdparty\" \"zero_3rdparty docs\"\n    click model_lib href \"/py-libs/model-lib\" \"model_lib docs\"\n    click docker_compose_parser href \"/py-libs/docker_compose_parser\" \"docker_compose_parser docs\"\n    click compose_chart_export href \"/py-libs/compose_chart_export\" \"compose_chart_export docs\"\n    click pants_py_deploy href \"/py-libs/_pants/pants_py_deploy\" \"pants_py_deploy docs\"</code></pre> <ul> <li>(Click) on a library to see the documentation</li> <li>The higher up in the hierarchy the more dependencies needs to be installed<ul> <li>e.g., <code>zero_3rdparty</code> has no dependencies and <code>pants_py_deploy</code> depends on all the others</li> </ul> </li> </ul>"},{"location":"#local-installation","title":"Local Installation","text":"<ul> <li>Install <code>just</code></li> <li>Install <code>uv</code></li> </ul> <pre><code>pre-commit install --hook-type pre-push\nuv sync\ncode .\n</code></pre>"},{"location":"#release-process","title":"Release process","text":"<ol> <li>Do changes on your branch</li> <li>Bump the versions you want to deploy <pre><code>just pkg-version z beta # alpha/patch,etc.\njust pkg-version m beta\n</code></pre></li> <li>Merge and wait for release to complete</li> </ol>"},{"location":"readme/","title":"Readme","text":""},{"location":"readme/#py-libs-drastic-changes-coming-for-v100","title":"py-libs (drastic changes coming for v1.0.0)","text":"<ul> <li>An experiment for sharing python packages</li> <li>~~compose_chart_export~~<ul> <li><code>pip install compose-chart-export</code></li> </ul> </li> <li>~~docker_compose_parser~~<ul> <li><code>pip install docker-compose-parser</code></li> </ul> </li> <li>model_lib-pydantic base models with convenient dump methods<ul> <li><code>pip install model-lib</code></li> </ul> </li> <li>zero_lib-handy standalone scripts without 3rdparty dependencies<ul> <li><code>pip install zero-3rdparty</code></li> </ul> </li> </ul>"},{"location":"readme/#hierarchy","title":"Hierarchy","text":"<pre><code>flowchart TD\n    model_lib --&gt; zero_3rdparty\n    compose_chart_export --&gt; model_lib\n    docker_compose_parser --&gt; model_lib\n    compose_chart_export --&gt; docker_compose_parser\n    pants_py_deploy --&gt; compose_chart_export\n    pants_py_deploy --&gt; docker_compose_parser\n\n    click zero_3rdparty href \"/py-libs/zero-3rdparty\" \"zero_3rdparty docs\"\n    click model_lib href \"/py-libs/model-lib\" \"model_lib docs\"\n    click docker_compose_parser href \"/py-libs/docker_compose_parser\" \"docker_compose_parser docs\"\n    click compose_chart_export href \"/py-libs/compose_chart_export\" \"compose_chart_export docs\"\n    click pants_py_deploy href \"/py-libs/_pants/pants_py_deploy\" \"pants_py_deploy docs\"</code></pre> <ul> <li>(Click) on a library to see the documentation</li> <li>The higher up in the hierarchy the more dependencies needs to be installed<ul> <li>e.g., <code>zero_3rdparty</code> has no dependencies and <code>pants_py_deploy</code> depends on all the others</li> </ul> </li> </ul>"},{"location":"readme/#local-installation","title":"Local Installation","text":"<ul> <li>Install <code>just</code></li> <li>Install <code>uv</code></li> </ul> <pre><code>pre-commit install --hook-type pre-push\nuv sync\ncode .\n</code></pre>"},{"location":"readme/#release-process","title":"Release process","text":"<ol> <li>Do changes on your branch</li> <li>Bump the versions you want to deploy <pre><code>just pkg-version z beta # alpha/patch,etc.\njust pkg-version m beta\n</code></pre></li> <li>Merge and wait for release to complete</li> </ol>"},{"location":"_pants/pants_py_deploy/","title":"Pants py_deploy plugin","text":"<ul> <li>Purpose: Make it easy to maintain docker-compose files and helm-charts across projects</li> <li>Goals</li> <li>Support updating services.{service_name}.<ul> <li><code>environment</code> by scanning source code</li> <li><code>ports</code> by scanning source code</li> </ul> </li> <li>Support updating {chart}/<ul> <li><code>values.yaml</code></li> <li><code>{deployment|statefulset}.yaml</code></li> </ul> </li> <li>Use Pantsbuild pants to write a plugin that can be pip installed</li> <li>Then invoking it should be <code>pants fix ::</code></li> <li>Less pants boilerplate and support multi-platform (arm+amd) with a macro</li> <li>Enabling docker-compose|helm-chart templates by using the macro and adding fields to pex_binary</li> </ul>"},{"location":"_pants/pants_py_deploy/#draft-api","title":"Draft api","text":"<ul> <li><code>BUILD</code> file <pre><code>py_deploy(\n    name=\"app-name\",\n    entrypoint=package_name,\n    docker={},\n    helm={},# support more config in the future\n)\n</code></pre></li> <li>generates</li> <li>4 <code>pex_binary</code> targets<ul> <li>Each platform has one <code>pex_binary</code> for sources and one <code>pex_binary</code> for requirements</li> </ul> </li> <li>2 <code>docker_image</code> targets<ul> <li>Both pex_binaries are copied, copies requirements 1st to take advantage of docker build cache</li> </ul> </li> <li>1 <code>helm_chart</code> with a <code>resource</code> of all the files u</li> <li>Usage in <code>pants publish ::</code></li> <li>docker and helm repository = <code>app-name</code></li> <li>tags:, <code>app-name-amd</code>, <code>app-name-arm</code>, <code>app-name-chart</code></li> <li>Usage in <code>pants fix ::</code></li> <li><code>docker-compose.yaml</code> file &amp; <code>chart/*</code> are create if they don't exist</li> <li>if they exist: ensure environment &amp; ports are up-to-date</li> </ul>"},{"location":"_pants/pants_py_deploy/#later-ideas","title":"Later ideas","text":"<ul> <li>support analyzing env-vars and using them for adding services like postgres/rabbitmq/mongo etc. to the docker-compose file</li> </ul>"},{"location":"_pants/pants_py_deploy/readme/","title":"Pants py_deploy plugin","text":"<ul> <li>Purpose: Make it easy to maintain docker-compose files and helm-charts across projects</li> <li>Goals</li> <li>Support updating services.{service_name}.<ul> <li><code>environment</code> by scanning source code</li> <li><code>ports</code> by scanning source code</li> </ul> </li> <li>Support updating {chart}/<ul> <li><code>values.yaml</code></li> <li><code>{deployment|statefulset}.yaml</code></li> </ul> </li> <li>Use Pantsbuild pants to write a plugin that can be pip installed</li> <li>Then invoking it should be <code>pants fix ::</code></li> <li>Less pants boilerplate and support multi-platform (arm+amd) with a macro</li> <li>Enabling docker-compose|helm-chart templates by using the macro and adding fields to pex_binary</li> </ul>"},{"location":"_pants/pants_py_deploy/readme/#draft-api","title":"Draft api","text":"<ul> <li><code>BUILD</code> file <pre><code>py_deploy(\n    name=\"app-name\",\n    entrypoint=package_name,\n    docker={},\n    helm={},# support more config in the future\n)\n</code></pre></li> <li>generates</li> <li>4 <code>pex_binary</code> targets<ul> <li>Each platform has one <code>pex_binary</code> for sources and one <code>pex_binary</code> for requirements</li> </ul> </li> <li>2 <code>docker_image</code> targets<ul> <li>Both pex_binaries are copied, copies requirements 1st to take advantage of docker build cache</li> </ul> </li> <li>1 <code>helm_chart</code> with a <code>resource</code> of all the files u</li> <li>Usage in <code>pants publish ::</code></li> <li>docker and helm repository = <code>app-name</code></li> <li>tags:, <code>app-name-amd</code>, <code>app-name-arm</code>, <code>app-name-chart</code></li> <li>Usage in <code>pants fix ::</code></li> <li><code>docker-compose.yaml</code> file &amp; <code>chart/*</code> are create if they don't exist</li> <li>if they exist: ensure environment &amp; ports are up-to-date</li> </ul>"},{"location":"_pants/pants_py_deploy/readme/#later-ideas","title":"Later ideas","text":"<ul> <li>support analyzing env-vars and using them for adding services like postgres/rabbitmq/mongo etc. to the docker-compose file</li> </ul>"},{"location":"ask-shell/","title":"Ask-Shell - Build Pretty, Helpful, and Testable CLIs","text":"<p>Documentation will come soon!</p>"},{"location":"ask-shell/CHANGELOG/","title":"Changelog","text":""},{"location":"ask-shell/CHANGELOG/#021-2025-10-22t06-49z","title":"0.2.1 2025-10-22T06-49Z","text":""},{"location":"ask-shell/CHANGELOG/#console","title":"Console","text":"<ul> <li>fix: Adds flag for skip_rich_exception instead of hard-coded 123bf4</li> </ul>"},{"location":"ask-shell/CHANGELOG/#020-2025-10-19t17-16z","title":"0.2.0 2025-10-19T17-16Z","text":""},{"location":"ask-shell/CHANGELOG/#root","title":"Root","text":"<ul> <li>New class AskShellSettings</li> </ul>"},{"location":"ask-shell/CHANGELOG/#ask","title":"Ask","text":"<ul> <li>New function confirm</li> <li>New function text</li> <li>New function select_list_multiple</li> <li>New function select_list_multiple_choices</li> <li>New function select_dict</li> <li>New function select_list</li> <li>New function select_list_choice</li> <li>New class NewHandlerChoice</li> <li>New class ChoiceTyped</li> <li>New class SelectOptions</li> <li>New class KeyInput</li> <li>New class PromptMatch</li> <li>New class question_patcher</li> <li>New class force_interactive</li> <li>New class raise_on_question</li> <li>New exception RaiseOnQuestionError</li> </ul>"},{"location":"ask-shell/CHANGELOG/#console_1","title":"Console","text":"<ul> <li>New function configure_logging</li> <li>New function add_renderable</li> <li>New function get_live_console</li> <li>New function print_to_live</li> <li>New function log_to_live</li> <li>New class RemoveLivePart</li> <li>New function interactive_shell</li> <li>New class new_task</li> </ul>"},{"location":"ask-shell/CHANGELOG/#shell","title":"Shell","text":"<ul> <li>New function kill</li> <li>New function kill_all_runs</li> <li>New function stop_runs_and_pool</li> <li>New function run</li> <li>New function run_and_wait</li> <li>New function run_error</li> <li>New function wait_on_ok_errors</li> <li>New class ShellRun</li> <li>New class run_pool</li> <li>New class ShellConfig</li> <li>New class handle_interrupt_wait</li> <li>New exception ShellError</li> </ul>"},{"location":"ask-shell/CHANGELOG/#shell_events","title":"Shell_Events","text":"<ul> <li>New class ShellRunStdStarted</li> <li>New class ShellRunStdOutput</li> <li>New class ShellRunPOpenStarted</li> <li>New class ShellRunStdReadError</li> <li>New class ShellRunRetryAttempt</li> <li>New class ShellRunBefore</li> <li>New class ShellRunAfter</li> <li>New type_alias OutputCallbackT</li> <li>New type_alias ShellRunCallbackT</li> </ul>"},{"location":"ask-shell/readme/","title":"Ask-Shell - Build Pretty, Helpful, and Testable CLIs","text":"<p>Documentation will come soon!</p>"},{"location":"compose_chart_export/","title":"Compose Chart Export","text":"<ul> <li>Uses <code>docker-compose.yaml</code> to generate a helm chart</li> <li>Supports various customizations using labels</li> </ul>"},{"location":"compose_chart_export/readme/","title":"Compose Chart Export","text":"<ul> <li>Uses <code>docker-compose.yaml</code> to generate a helm chart</li> <li>Supports various customizations using labels</li> </ul>"},{"location":"docker_compose_parser/","title":"Docker Compose Parser","text":"<ul> <li>Simple module for parsing, modifying, and export <code>docker-compose</code> files</li> </ul>"},{"location":"docker_compose_parser/readme/","title":"Docker Compose Parser","text":"<ul> <li>Simple module for parsing, modifying, and export <code>docker-compose</code> files</li> </ul>"},{"location":"model-lib/","title":"Model-lib - pydantic base models with convenient dump methods","text":""},{"location":"model-lib/#installation","title":"Installation","text":"<p><code>pip install 'model-lib[full]'</code></p>"},{"location":"model-lib/#model-lib-tutorial-what-classes-to-use-as-base-classes-how-to-serialize-them-and-add-metadata","title":"Model-lib tutorial: What classes to use as base classes, how to serialize them, and add metadata","text":"<ul> <li>A library built on top of pydantic</li> <li>Both pydantic v1 and v2 are supported</li> <li>The models: <code>Event</code> and <code>Entity</code> are subclassing pydantic.BaseModel<ul> <li>Event is immutable</li> <li>Entity is mutable</li> <li>The specific configuration are:<ul> <li>Automatic registering for dumping to the various formats</li> <li>Support different serializers for yaml/json/pretty_json/toml</li> <li>use_enum_values</li> <li>see model_base for details</li> </ul> </li> </ul> </li> <li>Use <code>dump(model|payload, format) -&gt; str</code></li> <li>if using an <code>Event|Entity</code> it should \"just-work\"</li> <li>Alternatively, support custom dumping with <code>register_dumper(instance_type: Type[T],dump_call: DumpCall)</code> (see example below)</li> <li>Use <code>parse_payload(payload, format)</code> to parse to a <code>dict</code> or <code>list</code></li> <li>bytes</li> <li>str</li> <li>pathlib.Path (format not necessary if file has extension: <code>.yaml|.yml|json|toml</code>)</li> <li>dict|list will be returned directly</li> <li>supports <code>register_parser</code> for adding e.g., a parser for KafkaMessage</li> <li>Use <code>parse_model(payload, t=Type, format)</code> to parse and create a model</li> <li><code>t</code> not necessary if class name stored in <code>metadata.model_name</code> (see example below)</li> <li>format not necessary if parsing from a file with extension</li> </ul> <pre><code>from datetime import datetime\n\nfrom freezegun import freeze_time\nfrom pydantic import Field\n\nfrom model_lib import (\n    Entity,\n    Event,\n    dump,\n    dump_with_metadata,\n    parse_model,\n    FileFormat,\n    register_dumper,\n)\nfrom model_lib.serialize.parse import register_parser, parse_payload\n\ndump_formats = list(FileFormat)\nexpected_dump_formats: list[str] = [\n    \"json\",\n    \"pretty_json\",\n    \"yaml\",\n    \"yml\",\n    \"json_pydantic\",\n    \"pydantic_json\",\n    \"toml\",\n    \"toml_compact\",\n]\nmissing_dump_formats = set(FileFormat) - set(expected_dump_formats)\nassert not missing_dump_formats, f\"found missing dump formats: {missing_dump_formats}\"\n\n\nclass Birthday(Event):\n    \"\"\"\n    &gt;&gt;&gt; birthday = Birthday()\n    \"\"\"\n\n    date: datetime = Field(default_factory=datetime.utcnow)\n\n\nclass Person(Entity):\n    \"\"\"\n    &gt;&gt;&gt; person = Person(name=\"espen\", age=99)\n    &gt;&gt;&gt; person.age += 1 # mutable\n    &gt;&gt;&gt; person.age\n    100\n    \"\"\"\n\n    name: str\n    age: int\n\n\n_pretty_person = \"\"\"\\\n{\n  \"age\": 99,\n  \"name\": \"espen\"\n}\"\"\"\n\n\ndef test_show_dumping():\n    with freeze_time(\"2020-01-01\"):\n        birthday = Birthday(date=datetime.utcnow())\n        # can dump non-primitives e.g., datetime\n        assert dump(birthday, \"json\") == '{\"date\":\"2020-01-01T00:00:00\"}'\n    person = Person(name=\"espen\", age=99)\n    assert dump(person, \"yaml\") == \"name: espen\\nage: 99\\n\"\n    assert dump(person, \"pretty_json\") == _pretty_person\n\n\n_metadata_dump = \"\"\"\\\nmodel:\n  name: espen\n  age: 99\nmetadata:\n  model_name: Person\n\"\"\"\n\n\ndef test_show_parsing(tmp_path):\n    path_json = tmp_path / \"example.json\"\n    path_json.write_text(_pretty_person)\n    person = Person(name=\"espen\", age=99)\n    assert parse_model(path_json, t=Person) == person\n    assert dump_with_metadata(person, format=\"yaml\") == _metadata_dump\n    path_yaml = tmp_path / \"example.yaml\"\n    path_yaml.write_text(_metadata_dump)\n    assert parse_model(path_yaml) == person  # metadata is used to find the class\n\n\nclass CustomDumping:\n    def __init__(self, first_name: str, last_name: str):\n        self.first_name = first_name\n        self.last_name = last_name\n\n    def __eq__(self, other):\n        if isinstance(other, CustomDumping):\n            return self.__dict__ == other.__dict__\n        return super().__eq__(other)\n\n\ndef custom_dump(custom: CustomDumping) -&gt; dict:\n    return dict(full_name=f\"{custom.first_name} {custom.last_name}\")\n\n\nregister_dumper(CustomDumping, custom_dump)\n\n\nclass CustomKafkaPayload:\n    def __init__(self, body: str, topic: str):\n        self.topic = topic\n        self.body = body\n\n\ndef custom_parse_kafka(payload: CustomKafkaPayload, format: str) -&gt; dict | list: # use Union[dict, list] if py3.9\n    return parse_payload(payload.body, format)\n\n\nregister_parser(CustomKafkaPayload, custom_parse_kafka)\n\n\ndef test_custom_dump():\n    instance = CustomDumping(\"Espen\", \"Python\")\n    assert dump(instance, \"json\") == '{\"full_name\":\"Espen Python\"}'\n    payload = CustomKafkaPayload(\n        body='{\"first_name\": \"Espen\", \"last_name\": \"Python\"}', topic=\"some-topic\"\n    )\n    assert parse_model(payload, t=CustomDumping) == instance\n</code></pre>"},{"location":"model-lib/readme/","title":"Model-lib - pydantic base models with convenient dump methods","text":""},{"location":"model-lib/readme/#installation","title":"Installation","text":"<p><code>pip install 'model-lib[full]'</code></p>"},{"location":"model-lib/readme/#model-lib-tutorial-what-classes-to-use-as-base-classes-how-to-serialize-them-and-add-metadata","title":"Model-lib tutorial: What classes to use as base classes, how to serialize them, and add metadata","text":"<ul> <li>A library built on top of pydantic</li> <li>Both pydantic v1 and v2 are supported</li> <li>The models: <code>Event</code> and <code>Entity</code> are subclassing pydantic.BaseModel<ul> <li>Event is immutable</li> <li>Entity is mutable</li> <li>The specific configuration are:<ul> <li>Automatic registering for dumping to the various formats</li> <li>Support different serializers for yaml/json/pretty_json/toml</li> <li>use_enum_values</li> <li>see model_base for details</li> </ul> </li> </ul> </li> <li>Use <code>dump(model|payload, format) -&gt; str</code></li> <li>if using an <code>Event|Entity</code> it should \"just-work\"</li> <li>Alternatively, support custom dumping with <code>register_dumper(instance_type: Type[T],dump_call: DumpCall)</code> (see example below)</li> <li>Use <code>parse_payload(payload, format)</code> to parse to a <code>dict</code> or <code>list</code></li> <li>bytes</li> <li>str</li> <li>pathlib.Path (format not necessary if file has extension: <code>.yaml|.yml|json|toml</code>)</li> <li>dict|list will be returned directly</li> <li>supports <code>register_parser</code> for adding e.g., a parser for KafkaMessage</li> <li>Use <code>parse_model(payload, t=Type, format)</code> to parse and create a model</li> <li><code>t</code> not necessary if class name stored in <code>metadata.model_name</code> (see example below)</li> <li>format not necessary if parsing from a file with extension</li> </ul> <pre><code>from datetime import datetime\n\nfrom freezegun import freeze_time\nfrom pydantic import Field\n\nfrom model_lib import (\n    Entity,\n    Event,\n    dump,\n    dump_with_metadata,\n    parse_model,\n    FileFormat,\n    register_dumper,\n)\nfrom model_lib.serialize.parse import register_parser, parse_payload\n\ndump_formats = list(FileFormat)\nexpected_dump_formats: list[str] = [\n    \"json\",\n    \"pretty_json\",\n    \"yaml\",\n    \"yml\",\n    \"json_pydantic\",\n    \"pydantic_json\",\n    \"toml\",\n    \"toml_compact\",\n]\nmissing_dump_formats = set(FileFormat) - set(expected_dump_formats)\nassert not missing_dump_formats, f\"found missing dump formats: {missing_dump_formats}\"\n\n\nclass Birthday(Event):\n    \"\"\"\n    &gt;&gt;&gt; birthday = Birthday()\n    \"\"\"\n\n    date: datetime = Field(default_factory=datetime.utcnow)\n\n\nclass Person(Entity):\n    \"\"\"\n    &gt;&gt;&gt; person = Person(name=\"espen\", age=99)\n    &gt;&gt;&gt; person.age += 1 # mutable\n    &gt;&gt;&gt; person.age\n    100\n    \"\"\"\n\n    name: str\n    age: int\n\n\n_pretty_person = \"\"\"\\\n{\n  \"age\": 99,\n  \"name\": \"espen\"\n}\"\"\"\n\n\ndef test_show_dumping():\n    with freeze_time(\"2020-01-01\"):\n        birthday = Birthday(date=datetime.utcnow())\n        # can dump non-primitives e.g., datetime\n        assert dump(birthday, \"json\") == '{\"date\":\"2020-01-01T00:00:00\"}'\n    person = Person(name=\"espen\", age=99)\n    assert dump(person, \"yaml\") == \"name: espen\\nage: 99\\n\"\n    assert dump(person, \"pretty_json\") == _pretty_person\n\n\n_metadata_dump = \"\"\"\\\nmodel:\n  name: espen\n  age: 99\nmetadata:\n  model_name: Person\n\"\"\"\n\n\ndef test_show_parsing(tmp_path):\n    path_json = tmp_path / \"example.json\"\n    path_json.write_text(_pretty_person)\n    person = Person(name=\"espen\", age=99)\n    assert parse_model(path_json, t=Person) == person\n    assert dump_with_metadata(person, format=\"yaml\") == _metadata_dump\n    path_yaml = tmp_path / \"example.yaml\"\n    path_yaml.write_text(_metadata_dump)\n    assert parse_model(path_yaml) == person  # metadata is used to find the class\n\n\nclass CustomDumping:\n    def __init__(self, first_name: str, last_name: str):\n        self.first_name = first_name\n        self.last_name = last_name\n\n    def __eq__(self, other):\n        if isinstance(other, CustomDumping):\n            return self.__dict__ == other.__dict__\n        return super().__eq__(other)\n\n\ndef custom_dump(custom: CustomDumping) -&gt; dict:\n    return dict(full_name=f\"{custom.first_name} {custom.last_name}\")\n\n\nregister_dumper(CustomDumping, custom_dump)\n\n\nclass CustomKafkaPayload:\n    def __init__(self, body: str, topic: str):\n        self.topic = topic\n        self.body = body\n\n\ndef custom_parse_kafka(payload: CustomKafkaPayload, format: str) -&gt; dict | list: # use Union[dict, list] if py3.9\n    return parse_payload(payload.body, format)\n\n\nregister_parser(CustomKafkaPayload, custom_parse_kafka)\n\n\ndef test_custom_dump():\n    instance = CustomDumping(\"Espen\", \"Python\")\n    assert dump(instance, \"json\") == '{\"full_name\":\"Espen Python\"}'\n    payload = CustomKafkaPayload(\n        body='{\"first_name\": \"Espen\", \"last_name\": \"Python\"}', topic=\"some-topic\"\n    )\n    assert parse_model(payload, t=CustomDumping) == instance\n</code></pre>"},{"location":"pkg-ext/pkg_ext/testdata/e2e/01_initial/CHANGELOG/","title":"Changelog","text":""},{"location":"pkg-ext/pkg_ext/testdata/e2e/01_initial/CHANGELOG/#010-2025-10-18-211306123450000","title":"0.1.0 2025-10-18 21:13:06.12345+00:00","text":""},{"location":"pkg-ext/pkg_ext/testdata/e2e/01_initial/CHANGELOG/#my_dep","title":"My_Dep","text":"<ul> <li>New class MyCls</li> </ul>"},{"location":"pkg-ext/pkg_ext/testdata/e2e/01_initial/CHANGELOG/#my_group","title":"My_Group","text":"<ul> <li>New function expose</li> <li>New function expose_with_arg</li> </ul>"},{"location":"pkg-ext/pkg_ext/testdata/e2e/02_dep_order/CHANGELOG/","title":"Changelog","text":""},{"location":"pkg-ext/pkg_ext/testdata/e2e/02_dep_order/CHANGELOG/#010-2025-10-18-211306123450000","title":"0.1.0 2025-10-18 21:13:06.12345+00:00","text":""},{"location":"pkg-ext/pkg_ext/testdata/e2e/02_dep_order/CHANGELOG/#g1","title":"G1","text":"<ul> <li>New function a</li> <li>New function b</li> <li>New function c</li> </ul>"},{"location":"pkg-ext/pkg_ext/testdata/e2e/03_nested/CHANGELOG/","title":"Changelog","text":""},{"location":"pkg-ext/pkg_ext/testdata/e2e/03_nested/CHANGELOG/#010-2025-10-18-211306123450000","title":"0.1.0 2025-10-18 21:13:06.12345+00:00","text":""},{"location":"pkg-ext/pkg_ext/testdata/e2e/03_nested/CHANGELOG/#n1","title":"N1","text":"<ul> <li>New function a</li> <li>New function b</li> </ul>"},{"location":"pkg-ext/pkg_ext/testdata/e2e/04_git_fix/CHANGELOG/","title":"Changelog","text":""},{"location":"pkg-ext/pkg_ext/testdata/e2e/04_git_fix/CHANGELOG/#011-2025-10-18-211306123450000","title":"0.1.1 2025-10-18 21:13:06.12345+00:00","text":""},{"location":"pkg-ext/pkg_ext/testdata/e2e/04_git_fix/CHANGELOG/#root","title":"Root","text":"<ul> <li>fix: adds chosen file (GIT_SHA)</li> </ul>"},{"location":"pkg-ext/pkg_ext/testdata/e2e/04_git_fix/CHANGELOG/#010-2025-10-18-211306123450000","title":"0.1.0 2025-10-18 21:13:06.12345+00:00","text":""},{"location":"pkg-ext/pkg_ext/testdata/e2e/04_git_fix/CHANGELOG/#git_inferred","title":"Git_Inferred","text":"<ul> <li>New function inferred</li> </ul>"},{"location":"zero-3rdparty/","title":"Zero 3rd-party","text":"<ul> <li>May add some examples here...</li> </ul>"},{"location":"zero-3rdparty/readme/","title":"Zero 3rd-party","text":"<ul> <li>May add some examples here...</li> </ul>"}]}